
# How To Create A Data Pipeline Automation

Cr√©er une **automatisation d‚Äôun pipeline de donn√©es** signifie concevoir un processus qui collecte, transforme et stocke les donn√©es sans intervention humaine manuelle. Voici une approche structur√©e, claire et pratique pour concevoir ce pipeline automatis√©. 


Plus d'infos : https://estuary.dev/blog/data-pipeline-automation/

---

## üß© √âtapes cl√©s pour automatiser un pipeline de donn√©es

### 1. **D√©finir les objectifs et les sources de donn√©es**

* Quels types de donn√©es traites-tu ? (logs, fichiers CSV, API, base de donn√©es, etc.)
* Quelle est la fr√©quence d'actualisation ? (temps r√©el, chaque heure, chaque jour)
* O√π les donn√©es doivent-elles √™tre stock√©es ? (data warehouse, S3, PostgreSQL‚Ä¶)

### 2. **Extraire les donn√©es (ETL : Extract)**

Utilise des connecteurs adapt√©s :

* API REST : via `requests` (Python)
* Base de donn√©es : via `SQLAlchemy`, `psycopg2`, etc.
* Fichiers : via `pandas.read_csv`, `read_excel`, etc.

**Exemple en Python (API REST):**

```python
import requests

response = requests.get("https://api.example.com/data")
data = response.json()
```

---

### 3. **Transformer les donn√©es (Transform)**

* Nettoyage : suppression des doublons, gestion des valeurs nulles
* Normalisation : formats de date, unit√©s
* Enrichissement : jointures avec d'autres sources

**Exemple en Python avec Pandas :**

```python
import pandas as pd

df = pd.DataFrame(data)
df.drop_duplicates(inplace=True)
df['date'] = pd.to_datetime(df['date'])
```

---

### 4. **Charger les donn√©es (Load)**

* Charger dans une base de donn√©es, un data lake ou un fichier cible

**Exemple : √©crire vers une base PostgreSQL**

```python
from sqlalchemy import create_engine

engine = create_engine('postgresql://user:password@host:port/dbname')
df.to_sql('table_cible', engine, if_exists='replace', index=False)
```

---

### 5. **Orchestration & automatisation**

Utilise des outils d‚Äôorchestration pour planifier et automatiser les t√¢ches :

#### üîß Outils courants :

| Outil              | Cas d‚Äôusage                          |
| ------------------ | ------------------------------------ |
| **Apache Airflow** | Pipelines complexes avec d√©pendances |
| **Cron + Scripts** | Petits jobs simples et r√©currents    |
| **Prefect**        | Alternatif moderne √† Airflow         |
| **Dagster**        | Framework moderne orient√© DataOps    |

**Exemple avec Airflow (DAG) :**

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def extract(): ...
def transform(): ...
def load(): ...

with DAG("data_pipeline", start_date=datetime(2024, 1, 1), schedule_interval='@daily') as dag:
    t1 = PythonOperator(task_id='extract', python_callable=extract)
    t2 = PythonOperator(task_id='transform', python_callable=transform)
    t3 = PythonOperator(task_id='load', python_callable=load)

    t1 >> t2 >> t3  # D√©pendances
```

---

### 6. **Surveillance et alertes**

* Logs (Airflow, ELK, etc.)
* Alertes : email, Slack, Grafana + Prometheus

---

## ‚úÖ R√©sum√© visuel du pipeline automatis√©

```
[SOURCE] --> [EXTRACTION] --> [TRANSFORMATION] --> [CHARGEMENT] --> [STOCKAGE]
                     \       /                \     /              \       /
                 [Code Python]         [Airflow / Cron Job]      [DB / S3 / Data Lake]
```

---

## Tu veux aller plus loin ?

Je peux t'aider √† :

* Cr√©er un projet Airflow √©tape par √©tape
* Mettre en place un pipeline CI/CD pour tes scripts ETL
* Conteneuriser le pipeline avec Docker

Souhaites-tu un exemple complet pr√™t √† l‚Äôemploi ?



Parfait ! Voici un **guide pas √† pas** pour cr√©er et automatiser un **pipeline de donn√©es simple** sur **Ubuntu**, adapt√© aux d√©butants.

Nous allons utiliser :

* **Python** pour √©crire les √©tapes du pipeline (extraction, transformation, chargement)
* **Airflow** pour l'automatisation (orchestration)
* **PostgreSQL** comme base de donn√©es de destination
* **Ubuntu** comme environnement de travail

---

## üîß √âtape 1 ‚Äì Pr√©parer l‚Äôenvironnement (Ubuntu)

### 1.1. Mettre √† jour Ubuntu

```bash
sudo apt update && sudo apt upgrade -y
```

### 1.2. Installer Python, pip et venv

```bash
sudo apt install python3 python3-pip python3-venv -y
```

---

## üìÅ √âtape 2 ‚Äì Cr√©er un projet local Python

### 2.1. Cr√©er un dossier de projet

```bash
mkdir ~/projet_pipeline
cd ~/projet_pipeline
python3 -m venv venv
source venv/bin/activate
```

### 2.2. Installer les biblioth√®ques n√©cessaires

```bash
pip install pandas requests sqlalchemy psycopg2-binary apache-airflow
```

---

## üì¶ √âtape 3 ‚Äì √âcrire le pipeline (ETL)

Cr√©e un fichier `pipeline.py` :

```bash
touch pipeline.py
```

Et ajoute ce code :

```python
import requests
import pandas as pd
from sqlalchemy import create_engine

def extract():
    url = "https://jsonplaceholder.typicode.com/users"
    response = requests.get(url)
    data = response.json()
    return pd.DataFrame(data)

def transform(df):
    df = df[['id', 'name', 'email']]
    df.rename(columns={"id": "user_id"}, inplace=True)
    return df

def load(df):
    engine = create_engine("postgresql://postgres:password@localhost:5432/ma_base")
    df.to_sql("utilisateurs", engine, if_exists="replace", index=False)

def pipeline():
    df = extract()
    df = transform(df)
    load(df)

if __name__ == "__main__":
    pipeline()
```

**‚ö†Ô∏è** Remplace `password` et `ma_base` par tes vrais identifiants PostgreSQL.

---

## üóÉÔ∏è √âtape 4 ‚Äì Installer PostgreSQL localement

```bash
sudo apt install postgresql postgresql-contrib -y
sudo -u postgres psql
```

Dans le terminal PostgreSQL :

```sql
CREATE DATABASE ma_base;
CREATE USER mon_user WITH PASSWORD 'password';
GRANT ALL PRIVILEGES ON DATABASE ma_base TO mon_user;
\q
```

Ensuite, mets √† jour l‚ÄôURL de connexion dans `pipeline.py` :

```python
create_engine("postgresql://mon_user:password@localhost:5432/ma_base")
```

---

## üïê √âtape 5 ‚Äì Automatiser avec Airflow

### 5.1. Initialiser Airflow

```bash
export AIRFLOW_HOME=~/airflow
pip install apache-airflow
airflow db init
```

### 5.2. Cr√©er un utilisateur Airflow

```bash
airflow users create \
  --username admin \
  --firstname Prenom \
  --lastname Nom \
  --role Admin \
  --email email@example.com \
  --password admin
```

### 5.3. D√©marrer Airflow

```bash
airflow webserver --port 8080
```

(Ouvre [http://localhost:8080](http://localhost:8080) dans ton navigateur)

Dans un nouveau terminal :

```bash
airflow scheduler
```

---

## üß© √âtape 6 ‚Äì Cr√©er un DAG Airflow

Cr√©e le dossier :

```bash
mkdir -p ~/airflow/dags
```

Cr√©e un fichier : `~/airflow/dags/etl_dag.py`

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
from pipeline import extract, transform, load

def run_etl():
    df = extract()
    df = transform(df)
    load(df)

with DAG(
    dag_id="mon_etl_dag",
    start_date=datetime(2024, 1, 1),
    schedule_interval="@daily",
    catchup=False
) as dag:
    etl_task = PythonOperator(
        task_id="run_etl",
        python_callable=run_etl
    )
```

Assure-toi que le fichier `pipeline.py` se trouve bien dans le m√™me dossier que ton DAG ou qu‚Äôil est dans le `PYTHONPATH`.

---

## ‚úÖ R√©sultat

Une fois le **scheduler** et le **webserver** lanc√©s :

* Connecte-toi √† [http://localhost:8080](http://localhost:8080)
* Active le DAG `mon_etl_dag`
* Il ex√©cutera chaque jour automatiquement ton pipeline.

---

## üìå √Ä suivre (si tu veux aller plus loin)

* Ajout d‚Äôalertes email si une t√¢che √©choue
* Monitoring avec Grafana
* Dockeriser le pipeline
* D√©ployer sur un serveur distant

Souhaites-tu que je t‚Äôaide √† cr√©er une version Docker ou √† ajouter une API ou interface ?

