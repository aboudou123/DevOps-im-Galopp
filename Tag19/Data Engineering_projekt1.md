Datenpipeline fÃ¼r Lkwâ€‘Logistik
Data Engineering Zoom Camp | Kohorte 2024 | Capstoneâ€‘Projekt

Autor: Koffitse Aboudou

> WICHTIG: Um die ProjektÃ¼bersicht zu Ã¼berspringen und direkt zum Setup zu gelangen, klicken Sie hier.

<img src="static/assets/trucks.jpeg" alt="Trucks" height="300" width="600">

Inhaltsverzeichnis
 Problemstellung
 Werkzeuge & Technologien
 Architekturdiagramm
 Datenquellen & Schema
 Orchestrierung mit Mage
 Dashboard & Visualisierung
 WeiterfÃ¼hrende Ideen & NÃ¤chste Schritte
 Danksagung & Credits

> HINWEIS: Um die ProjektÃ¼bersicht zu Ã¼berspringen und direkt zum Setup zu gelangen, klicken Sie hier.

Problemstellung

Die Notwendigkeit, fÃ¼r ein Lkwâ€‘Logistikunternehmen eine Datenpipeline und ein Dashboard aufzubauen, ergibt sich aus dem Anspruch, Informationen zu TransportvorgÃ¤ngen â€“ insbesondere umsatzrelevante Daten je Kunde â€“ effizient zu erfassen, zu verarbeiten und bereitzustellen. Durch die Implementierung einer durchgÃ¤ngigen Pipeline, die Datenerfassung, â€‘aufbereitung, â€‘analyse und Visualisierung umfasst, und in Kombination mit einem benutzerfreundlichen Dashboard fÃ¼r das Management erhÃ¤lt das Unternehmen belastbare Einblicke in Umsatzentwicklung, Kostenstrukturen und die operative Performance.

Dieses Projekt implementiert eine vollstÃ¤ndige ETLâ€‘Pipeline (Extract, Transform, Load), mit der die Datenverarbeitung eines Lkwâ€‘Logistikunternehmens systematisch automatisiert und verbessert wird. Zur Umsetzung werden mehrere leistungsfÃ¤hige Technologien und Tools eingesetzt:

â€¢ Infrastrukturbereitstellung mit Terraform: Terraform erstellt die erforderliche Cloudâ€‘Infrastruktur auf Google Cloud Storage und BigQuery und sorgt so fÃ¼r eine skalierbare und robuste Umgebung fÃ¼r Verarbeitung und Speicherung der Daten.

â€¢ Datenorchestrierung mit Mage: Mage, ein moderner Orchestrator fÃ¼r Datenworkflows, automatisiert den ETLâ€‘Prozess. Er lÃ¤dt eine CSVâ€‘Datei vom lokalen System, konvertiert sie in ein Parquetâ€‘Format und schreibt die Daten anschlieÃŸend nach Google Cloud Storage und BigQuery. Der gesamte Ablauf ist containerisiert und lÃ¤uft in Docker, was Konsistenz und einfache Bereitstellung sicherstellt.

â€¢ Analytik mit dbt: Mit dbt (Data Build Tool) werden analytische Transformationen auf den in BigQuery abgelegten Daten ausgefÃ¼hrt. Dadurch entstehen aussagekrÃ¤ftige Datenmodelle und Transformationen, die tiefere Einblicke in die Logistikprozesse ermÃ¶glichen.

â€¢ Visualisierung mit Looker Studio: AbschlieÃŸend wird Looker Studio genutzt, um interaktive und intuitiv bedienbare Dashboards zu erstellen. Diese Dashboards stellen zentrale Kennzahlen und Auswertungen bereit, sodass das Management Entscheidungen auf Basis nahezu in Echtzeit verfÃ¼gbarer Daten treffen kann.

Werkzeuge & Technologien
 Containerisierung: Docker
 Workflowâ€‘Orchestrierung: Mage
 Datentransformationen: DBT DataBuildTools
 Data Lake: Google Cloud Storage
 Data Warehouse: Google BigQuery
 Infrastructure as Code (IaC): Terraform
 Visualisierung: Looker Studio

Architekturdiagramm

<img src="static/assets/dataarchitecture.drawio.svg" alt="Data Architecture" height="300" width="600">

Datenquellen & Schema

Die Projektidee basiert auf einem Lkwâ€‘Flottenunternehmen, dessen Fahrer fÃ¼r unterschiedliche Auftraggeber tÃ¤tig sind. Um keine vertraulichen Informationen zu verwenden, wurden die Daten synthetisch erzeugt. Die in diesem Projekt verwendeten simulierten DatensÃ¤tze befinden sich in den Verzeichnissen data und seed. Vergleichbare Daten kÃ¶nnen mit dem Pythonâ€‘Skript datagenerator.py generiert werden.

Im Wesentlichen kommen zwei CSVâ€‘Dateien zum Einsatz:  
eine im lokalen Ordner data, der wÃ¶chentlich mit den Fahrten der vergangenen Woche aktualisiert wird, sowie eine statische Tabelle mit den Kundentarifen fÃ¼r die jeweiligen Fahrtarten.

tripdata:  
   Diese CSV enthÃ¤lt Fahrtdaten fÃ¼r die Lkwâ€‘Logistik und umfasst folgende Spalten:

    date: Datum des Datensatzes  
    driver: Name der Fahrerin bzw. des Fahrers  
    customer: Name des Kunden  
    hours: Anzahl der Stunden fÃ¼r die Fahrt  
    km: ZurÃ¼ckgelegte Strecke in Kilometern  

customerrates:  
   Diese CSV beschreibt unterschiedliche Tarife pro Kunde und enthÃ¤lt folgende Spalten:

    customer: Name des Kunden  
    hourcity: Stundensatz fÃ¼r Fahrten im Stadtgebiet  
    hourregular: Stundensatz fÃ¼r regulÃ¤re Fahrten  
    hourhy: Stundensatz fÃ¼r Autobahnfahrten  
    fsccity: Treibstoffzuschlag fÃ¼r Stadtfahrten  
    fscregular: Treibstoffzuschlag fÃ¼r regulÃ¤re Fahrten  
    fschy: Treibstoffzuschlag fÃ¼r Autobahnfahrten  
    hymileage: Kilometerpauschale fÃ¼r Autobahnstrecken  

Orchestrierung mit Mage

<img src="static/assets/pipeline.png" alt="Pipeline" height="600" width="300">

Mage arbeitet mit modularen Codeâ€‘BlÃ¶cken; in dieser Pipeline kommen Pythonâ€‘, SQLâ€‘ und dbtâ€‘BlÃ¶cke in der folgenden Reihenfolge zum Einsatz:

Laden der CSVâ€‘Datei aus dem lokalen Verzeichnis.  
DurchfÃ¼hrung der Transformationen und Erzeugen einer Parquetâ€‘Datei.  
Export der Parquetâ€‘Datei nach Google Cloud Storage.  
Anlegen einer External Table in BigQuery auf Basis der Parquetâ€‘Datei im Googleâ€‘Cloudâ€‘Storageâ€‘Bucket.  
Installation aller dbtâ€‘AbhÃ¤ngigkeiten mittels dbt deps.  
Einspielen der Datei customerrates als Seed in das dbtâ€‘Projekt.  
Erzeugung sÃ¤mtlicher dbtâ€‘Modelle:
   - Aufbau von Stagingâ€‘Modellen fÃ¼r beide Dateien, inklusive zusÃ¤tzlicher Spalten wie tripid und triptype fÃ¼r tripdata.  
   - Aufbau eines Coreâ€‘Modells durch Join beider Stagingâ€‘Modelle und Berechnung der UmsÃ¤tze gemÃ¤ÃŸ der kundenspezifischen Tariflogik.

Dashboard & Visualisierung

Auf Basis von Google Looker Studio wurde ein Dashboard entwickelt, das Ereignisse in der Lkwâ€‘Logistik visualisiert. Durch die Nutzung des nativen Visualisierungstools direkt aus BigQuery heraus profitiert das Dashboard von hoher Performance und geringer Latenz. Die enge Integration von Looker und BigQuery ermÃ¶glicht effiziente Datenabfragen und â€‘verarbeitung und fÃ¼hrt zu schnelleren Insights sowie einer flÃ¼ssigen Nutzererfahrung.

Dashboardâ€‘Link: HIER

<img src="static/assets/dashboard.png" alt="Chart1" height="300" width="600">

WeiterfÃ¼hrende Ideen & NÃ¤chste Schritte
â€¢ Einsatz eines grÃ¶ÃŸeren Datenvolumens  
â€¢ Aufteilung der customer_ratesâ€‘Dateien nach Jahr  
â€¢ Bei umfangreicheren Datenmengen Einsatz zusÃ¤tzlicher Tools wie z.â€¯B. dlt  
â€¢ Nutzung von Partitionierung und Clustering  

Danksagung & Credits

Besonderer Dank gilt DataTalksClub fÃ¼r die Begleitung im Data Engineering Zoom Camp der letzten zehn Wochen. Es war ein Privileg, Teil der Springâ€‘â€™24â€‘Kohorte zu sein â€“ unbedingt vorbeischauen!

> â€DataTalks.Club â€“ the place to talk about data! We are a community of people who are passionate about data. Join us to talk about everything related to data, to learn more about applied machine learning with our free courses and materials, to discuss the engineering aspects of data science and analytics, to chat about career options and learn tips and tricks for the job interviews, to discover new things and have fun!
>
> Our weekly events include:
>
> ğŸ‘¨ğŸ¼â€ğŸ’» Free courses and weekly study groups where you can start practicing within a friendly community of learners
>
> ğŸ”§ Workshops where you can get hands-on tutorials about technical topics
>
> âš™ï¸ Open-Source Spotlight, where you can discover open-source tools with a short demo video
>
> ğŸ™ Live Podcasts with practitioners where they share their experience (and the recordings too)
>
> ğŸ“º Webinars with slides, where we discuss technical aspects of data scienceâ€œ

Data Talks Club
